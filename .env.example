# ============================================================================
# CORE APPLICATION SETTINGS
# ============================================================================

# Server Configuration
MXF_PORT=3001
MXF_SERVER_URL=http://localhost:3001
MXF_API_URL=http://localhost:3001/api
NODE_ENV=development
TZ=America/Denver

# Dashboard
DASHBOARD_URL=http://localhost:8080

# Logging
LOG_LEVEL=info
AUDIT_LOG_PATH=logs/audit.log

# Event Queue Configuration
EVENT_QUEUE_ENABLED=false
EVENT_QUEUE_DELAY_MS=5
EVENT_QUEUE_BATCH_SIZE=10
EVENT_QUEUE_MAX_SIZE=1000
EVENT_QUEUE_MAX_RETRIES=3


# ============================================================================
# DATABASE CONFIGURATION
# ============================================================================

# MongoDB Connection
MONGODB_URI=mongodb://localhost:27017/mxf

# MongoDB Docker Configuration (when using docker-compose)
MONGODB_USERNAME=mxf_admin
MONGODB_PASSWORD=changeme_in_production
MONGODB_DATABASE=mxf

# Redis Configuration (when using docker-compose)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=changeme_redis_password


# ============================================================================
# SECURITY & AUTHENTICATION
# ============================================================================

# Core Security
JWT_SECRET=change-this-to-a-secure-random-string
ENCRYPTION_KEY=change-this-to-a-secure-random-string-at-least-32-bytes

# SDK Domain Key (REQUIRED for all SDK connections)
# This key authenticates that the SDK is authorized to connect to this MXF server
# Generate: openssl rand -hex 32
# Share securely with SDK users (never commit to version control)
MXF_DOMAIN_KEY=your-64-char-hex-domain-key-here

# MXP Protocol Encryption
MXP_ENCRYPTION_KEY=change-this-to-a-secure-passphrase-for-mxp-encryption
MXP_ENCRYPTION_SALT=mxf-default-salt
MXP_ENCRYPTION_ENABLED=true

# Audit Settings
REDACTION_ENABLED=true
RETENTION_DAYS=90

# ============================================================================
# AGENT CONFIGURATION
# ============================================================================

MAX_AGENT_CONNECTIONS=1000
AGENT_HEARTBEAT_INTERVAL=30000
CONNECTION_TIMEOUT=60000

# Performance
COMPRESSION_THRESHOLD=1024
MAX_MESSAGE_SIZE=1048576 # 1MB


# =============================================================================
# Code Execution (Docker + Bun)
# =============================================================================

# Timeout Configuration
CODE_EXEC_TIMEOUT_DEFAULT=5000          # Default execution timeout in ms
CODE_EXEC_TIMEOUT_MAX=30000             # Maximum allowed timeout in ms
CODE_EXEC_TIMEOUT_BUFFER=1000           # Buffer for container overhead in ms

# Resource Limits
CODE_EXEC_MEMORY_LIMIT=128              # Memory limit in MB
CODE_EXEC_CPU_LIMIT=0.5                 # CPU cores (fractional)
CODE_EXEC_MAX_OUTPUT=10240              # Max stdout before truncation in bytes

# Docker Image
CODE_EXEC_IMAGE=mxf/code-executor:latest   # Docker image name:tag


# ============================================================================
# ADVANCED FEATURES
# ============================================================================

# TOON Optimization
TOON_OPTIMIZATION_ENABLED=true          # Enable TOON format for arrays

# Prompt Auto-Compaction
PROMPT_COMPACTION_ENABLED=false          # Enable context compression
PROMPT_COMPACTION_TIERED_ENABLED=false   # Enable tiered compression
PROMPT_COMPACTION_RESIDUALS_ENABLED=false # Preserve high-importance messages
PROMPT_COMPACTION_BUDGET_ENABLED=false   # Enable token budget allocation
PROMPT_COMPACTION_CONDENSED_MODE=false   # Minimal tool documentation
PROMPT_COMPACTION_DEFAULT_BUDGET=8000    # Default token budget
PROMPT_COMPACTION_MAX_SYSTEM_PROMPT_TOKENS=4000
PROMPT_COMPACTION_RESIDUAL_THRESHOLD=60  # Importance score threshold
PROMPT_COMPACTION_RESIDUAL_MAX_PERCENT=20 # Max residual budget %
PROMPT_COMPACTION_TIER=0                 # Default tier (0-3)

# MCP Prompts Integration
MCP_PROMPTS_ENABLED=false                # Enable MCP prompts
MCP_PROMPTS_CACHE_TTL=300000             # Cache TTL (5 min)
MCP_PROMPTS_CACHE_MAX_ENTRIES=100
MCP_PROMPTS_CACHE_STRATEGY=lru
MCP_PROMPTS_REFRESH_INTERVAL=60000       # Refresh interval (1 min)
MCP_PROMPTS_TIMEOUT=5000                 # Request timeout
MCP_PROMPTS_TRACK_TOKENS=true
MCP_PROMPTS_MAX_RESOURCE_SIZE=1048576    # 1MB
MCP_PROMPTS_COMPRESS_RESOURCES=true
MCP_PROMPTS_ALLOWED_SCHEMES=file,https

# Workflow System
WORKFLOW_SYSTEM_ENABLED=false            # Enable workflow agents
WORKFLOW_EXECUTION_ENABLED=false         # Enable server-side execution
WORKFLOW_TEMPLATES_ENABLED=false         # Enable workflow templates
WORKFLOW_VALIDATION_ENABLED=false        # Enable workflow validation

# LSP-MCP Bridge
LSP_ENABLED=false                        # Enable LSP integration
LSP_TYPESCRIPT_ENABLED=true              # TypeScript language server
LSP_PYTHON_ENABLED=false                 # Python language server (pylsp)
LSP_GO_ENABLED=false                     # Go language server (gopls)
LSP_CACHING_ENABLED=true                 # Cache LSP responses

# Nested Learning / Memory Strata
MEMORY_STRATA_ENABLED=false              # Enable memory strata system
MEMORY_STRATA_SURPRISE_DETECTION=false   # Enable Titans-style surprise
MEMORY_STRATA_CONSOLIDATION=false        # Enable automatic consolidation
MEMORY_STRATA_PATTERN_DETECTION=false    # Enable pattern learning

# Memory Utility Learning System (MULS)
MEMORY_UTILITY_LEARNING_ENABLED=false    # Enable/disable MULS (disabled by default)
QVALUE_DEFAULT=0.5                        # Default Q-value for new memories
QVALUE_LEARNING_RATE=0.1                  # EMA learning rate (Î±)
RETRIEVAL_LAMBDA_DEFAULT=0.5              # Default lambda when phase not specified

# ORPAR Phase-Specific Lambdas
RETRIEVAL_LAMBDA_OBSERVATION=0.2          # Prioritize semantic accuracy
RETRIEVAL_LAMBDA_REASONING=0.5            # Balance explore/exploit
RETRIEVAL_LAMBDA_PLANNING=0.7             # Exploit proven patterns
RETRIEVAL_LAMBDA_ACTION=0.3               # Stay grounded for tools
RETRIEVAL_LAMBDA_REFLECTION=0.6           # Favor good assessment memories

# ORPAR-Memory Integration
ORPAR_MEMORY_INTEGRATION_ENABLED=false    # Master integration flag (off by default - opt-in)

# Phase-Strata weights
PHASE_STRATA_OBSERVATION_PRIMARY=working,short_term
PHASE_STRATA_REASONING_PRIMARY=episodic,semantic
PHASE_STRATA_PLANNING_PRIMARY=semantic,long_term

# Surprise thresholds
SURPRISE_HIGH_THRESHOLD=0.7
SURPRISE_MODERATE_THRESHOLD=0.4
SURPRISE_MAX_EXTRA_OBSERVATIONS=3

# Phase reward weights
PHASE_WEIGHT_OBSERVATION=0.15
PHASE_WEIGHT_REASONING=0.20
PHASE_WEIGHT_PLANNING=0.30
PHASE_WEIGHT_ACTION=0.25
PHASE_WEIGHT_REFLECTION=0.10

# Consolidation rules
CONSOLIDATION_PROMOTION_QVALUE=0.7
CONSOLIDATION_DEMOTION_QVALUE=0.3

# Decentralization Foundation (EXPERIMENTAL)
DECENTRALIZATION_ENABLED=false           # Enable P2P features
DECENTRALIZATION_PEER_DISCOVERY=false    # Enable peer discovery
DECENTRALIZATION_GOSSIP=false            # Enable gossip protocol
DECENTRALIZATION_DHT=false               # Enable DHT (future)
DECENTRALIZATION_CONSENSUS=false         # Enable consensus (future)

# Task DAG & Knowledge Graph
TASK_DAG_ENABLED=false                   # Enable Task DAG system
KNOWLEDGE_GRAPH_ENABLED=false            # Enable Knowledge Graph
KG_ORPAR_INTEGRATION_ENABLED=false       # Enable ORPAR integration

# Enable TensorFlow.js for ML capabilities
TENSORFLOW_ENABLED=false


# ============================================================================
# SYSTEMLLM CONFIGURATION
# ============================================================================

# SystemLLM Configuration (for ORPAR control loop, pattern learning, etc.)
# Master switch to enable/disable SystemLLM features
# Disabled by default - enable for advanced multi-agent coordination and ORPAR control loop
SYSTEMLLM_ENABLED=false

# Provider for SystemLLM operations (openrouter, azure-openai, openai, anthropic, etc.)
# SYSTEMLLM_PROVIDER=openrouter

# Default model for SystemLLM (provider-specific model name)
# SYSTEMLLM_DEFAULT_MODEL=google/gemini-2.5-flash

# Enable dynamic model selection based on complexity (recommended for OpenRouter only)
# SYSTEMLLM_DYNAMIC_MODEL_SELECTION=true

# ============================================================================
# LLM PROVIDER API KEYS
# ============================================================================

# OpenRouter - Access to 200+ models
# OPENROUTER_API_KEY=your-openrouter-key-here

# Anthropic - Claude models
# ANTHROPIC_API_KEY=your-anthropic-key-here

# OpenAI - GPT models
# OPENAI_API_KEY=your-openai-key-here

# Google AI - Gemini models
# GOOGLE_AI_API_KEY=your-google-ai-key-here

# Azure OpenAI - Enterprise GPT
AZURE_OPENAI_API_KEY=your-azure-key-here
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4-1-mini
AZURE_OPENAI_API_VERSION=2024-04-01-preview
AZURE_OPENAI_MODEL_NAME=gpt-4.1-mini
# Note: You can also configure Azure settings per-agent using providerOptions in agent config

# xAI - Grok models
# XAI_API_KEY=your-xai-key-here

# Ollama - Local models (no API key needed, just endpoint)
# OLLAMA_BASE_URL=http://localhost:11434

# ============================================================================
# NETWORK RECOVERY & RESILIENCE
# ============================================================================

# OpenRouter-specific retry settings
OPENROUTER_MAX_RETRIES=3
OPENROUTER_BASE_DELAY_MS=1000
OPENROUTER_MAX_DELAY_MS=30000
OPENROUTER_RETRY_MULTIPLIER=2
OPENROUTER_REQUEST_QUEUE_DELAY_MS=100

# Circuit breaker settings
OPENROUTER_CIRCUIT_BREAKER_THRESHOLD=5
OPENROUTER_CIRCUIT_BREAKER_COOLDOWN_MS=60000

# Timeout settings
OPENROUTER_REQUEST_TIMEOUT_MS=30000

# Graceful degradation
OPENROUTER_ENABLE_GRACEFUL_DEGRADATION=true
OPENROUTER_ENABLE_DETAILED_LOGGING=true
ENABLE_GRACEFUL_DEGRADATION=true

# ============================================================================
# MEILISEARCH SEMANTIC SEARCH
# ============================================================================

# Enable/disable Meilisearch integration for semantic memory search
ENABLE_MEILISEARCH=true
ENABLE_SEMANTIC_SEARCH=true

# Meilisearch connection (Docker Compose uses internal network)
MEILISEARCH_HOST=http://localhost:7700
MEILISEARCH_MASTER_KEY=changeme_generate_secure_key

# Meilisearch embedding configuration
# Provider: openrouter, openai, voyage, or azure-openai
MEILISEARCH_EMBEDDING_PROVIDER=openrouter
# For OpenRouter (proxies OpenAI models), use: openai/text-embedding-3-small or openai/text-embedding-3-large
# For OpenAI, use: text-embedding-3-small
# For Voyage AI, use: voyage-3-lite
MEILISEARCH_EMBEDDING_MODEL=openai/text-embedding-3-small
MEILISEARCH_EMBEDDING_DIMENSIONS=1536
MEILISEARCH_BATCH_SIZE=100
MEILISEARCH_HYBRID_RATIO=0.7

# Meilisearch performance tuning (Docker Compose)
MEILISEARCH_ENV=production
MEILI_MAX_INDEXING_MEMORY=2GB
MEILI_MAX_INDEXING_THREADS=4
MEILI_ENABLE_METRICS=false
MEILI_EXPERIMENTAL_ENABLE_VECTOR_STORE=true


# ============================================================================
# N8N WORKFLOW AUTOMATION
# ============================================================================

# Supports both self-hosted (free) and cloud deployments
# Self-hosted: Run n8n locally with Docker or npm (include /api/v1 path)
# Cloud: Use n8n.cloud and provide your cloud instance URL (e.g., https://your-instance.n8n.cloud/api/v1)
N8N_API_URL=http://localhost:5678/api/v1
N8N_API_KEY=

# n8n Docker Configuration (when using docker-compose)
N8N_HOST=localhost
N8N_PROTOCOL=http
N8N_BASIC_AUTH_ACTIVE=true
N8N_BASIC_AUTH_USER=admin
N8N_BASIC_AUTH_PASSWORD=changeme_n8n_password

# Optional: Webhook authentication credentials (for webhook-triggered workflows)
N8N_WEBHOOK_USERNAME=
N8N_WEBHOOK_PASSWORD=

# ============================================================================
# EXTERNAL SERVICES & INTEGRATIONS
# ============================================================================

# External MCP Servers
DISABLE_EXTERNAL_MCP_SERVERS=false

# NASA API Configuration
# Get your free API key at: https://api.nasa.gov/
# Used for space weather data (DONKI - Space Weather Database)
NASA_API_KEY=DEMO_KEY

# Slack Configuration
SLACK_USER_ID=
SLACK_PR_WEBHOOK_URL=
SLACK_COMMIT_WEBHOOK_URL=

# ============================================================================
# DEMO CONFIGURATION
# ============================================================================

# Personal Access Token for Demo Scripts (REQUIRED for running demos)
# Generated by: bun run server:cli -- demo:setup
# This creates a demo user and PAT automatically
MXF_DEMO_ACCESS_TOKEN=
